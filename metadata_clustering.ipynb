{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for semantic data. uses readability nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import nltk\n",
    "#import raw_data_utils as utils\n",
    "#import extract_features as extract\n",
    "#import clean_data, qm_clean\n",
    "import readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('results_delivery_test.json') as f:\n",
    "    tokens_mini = json.load(f)\n",
    "with open('stopwords.txt') as f:\n",
    "    stopwords = set(f.read().split())\n",
    "    \n",
    "reducedDim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_doc_name_iterator(doc_token_collection, by_sentence=False, indiv_tokens = False, stemmed=False,\n",
    "                     stopwords=None, filter_fn = lambda x: True, get_key = False, skip_form_letters=False, \n",
    "                     condense_form_letters=False):\n",
    "    form_letter_appearance=set()\n",
    "    for item in doc_token_collection:\n",
    "        k = item['isFormLetter']\n",
    "        if skip_form_letters:\n",
    "            if k[0:3]!=\"nfl\":\n",
    "                continue\n",
    "        if condense_form_letters:\n",
    "            if k[0:3]!=\"nfl\": \n",
    "                if k[0:3] not in form_letter_appearance:\n",
    "                    form_letter_appearance.add(k[0:3])\n",
    "                else:\n",
    "                    continue\n",
    "        yield item\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "with open('hedges_hyland2005.txt') as f:\n",
    "    hedges = f.read().split('\\n')\n",
    "modals = [['may', 'might'], ['can', 'cant', 'could', 'couldnt'], ['would', 'wouldnt'], ['should', 'shouldnt'], ['will', 'wont'],\n",
    "          ['must'], ['wish', 'wished'], ['need', 'needed'], ['want', 'wanted']]\n",
    "self_singular = ['i', 'ive', 'im', 'me', 'my', 'mine', 'myself']\n",
    "self_plural = ['we', 'weve', 'were', 'our', 'ours', 'ourselves', 'us']\n",
    "other_singular = ['you', 'your', 'youre']\n",
    "other_plural = ['they', 'theyre', 'their']\n",
    "negations =['no', 'not', 'none', 'nobody', 'nothing', 'neither', 'nowhere', 'never', 'doesnt', 'isnt', 'wasnt', 'shouldnt', 'wouldnt', 'couldnt', 'wont', 'cant', 'dont']\n",
    "\n",
    "'''ADD: \n",
    "ALL THE OTHER FEATURES FROM THE LIST LOL\n",
    "negation\n",
    "average length\n",
    " total length\n",
    " contains keyword\n",
    " readability ease\n",
    " swearwords corpus\n",
    " if form letter\n",
    " sentiment words, \n",
    " capitalization\n",
    " punctuation\n",
    " POS tags?\n",
    "##ratio of xth pronoun over all pronouns.\n",
    "\n",
    "percent of sentences with 1st singular, 1st plural, 2nd singular pronouns \n",
    "average number of words per sentence, along with number of sentences\n",
    "percent of sentences with negation, with imperatives, with must, should, will, may, can etc. that identify various attitudes\n",
    "Flesch-Kincaid Reading Ease score, and frequency of swear words (fairly powerful in identifying legal terminology)\n",
    "Mention of critical people and organizations â€“ while these were hard coded, it should be possible to extract form the NER tags.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def word_freq_sentence(word, sentence):\n",
    "    return sentence.count(word) / total_words(sentence)\n",
    "\n",
    "def word_freq_comment(word, comment):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for sentence in comment:\n",
    "        count += sentence.count(word)\n",
    "        total += total_words(sentence)\n",
    "    return count / total\n",
    "    \n",
    "def num_words(sentence):\n",
    "    return sentence.count(' ') + 1\n",
    "\n",
    "def pronoun_freq_self(comment):\n",
    "    singular = 0\n",
    "    plural = 0\n",
    "    total_words = 0\n",
    "    for sentence in comment:\n",
    "        words = sentence.split()\n",
    "        total_words += num_words(sentence)\n",
    "        for word in words:\n",
    "            if word in self_singular:\n",
    "                singular += 1\n",
    "            if word in self_plural:\n",
    "                plural += 1\n",
    "    total = singular + plural\n",
    "    if total == 0:\n",
    "        return [0, 0]\n",
    "    return [(singular - plural) / total, total / total_words]\n",
    "\n",
    "def pronoun_freq_others(comment):\n",
    "    singular = 0\n",
    "    plural = 0\n",
    "    total_words = 0\n",
    "    for sentence in comment:\n",
    "        total_words += num_words(sentence)\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            if word in other_singular:\n",
    "                singular += 1\n",
    "            if word in other_plural:\n",
    "                plural += 1\n",
    "    total = singular + plural\n",
    "    if total == 0:\n",
    "        return [0, 0]\n",
    "    return [(singular - plural) / total, total / total_words]\n",
    "    \n",
    "def hedge_freq(comment):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    hedge_counts = [0] * len(hedges)\n",
    "    for sentence in comment:\n",
    "        words = sentence.split()\n",
    "        total += num_words(sentence)\n",
    "        for word in words:\n",
    "            if word in hedges:\n",
    "                count += 1\n",
    "                hedge_counts[hedges.index(word)] += 1\n",
    "    return [count / total, hedge_counts]\n",
    "\n",
    "def negation_freq(comment):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    neg_counts = [0] * len(negations)\n",
    "    for sentence in comment:\n",
    "        words = sentence.split()\n",
    "        total += num_words(sentence)\n",
    "        for word in words:\n",
    "            if word in negations:\n",
    "                count += 1\n",
    "                neg_counts[negations.index(word)] += 1\n",
    "    return [count / total, neg_counts]\n",
    "\n",
    "def modal_freq(comment):\n",
    "    modal_counts = [0] * len(modals)\n",
    "    total = 0\n",
    "    count_tot = 0\n",
    "    for sentence in comment:\n",
    "        words = sentence.split()\n",
    "        total += num_words(sentence)\n",
    "        for word in words:\n",
    "            for i, modal_type in enumerate(modals):\n",
    "                if word in modal_type:\n",
    "                    count_tot += 1\n",
    "                    modal_counts[i] += 1\n",
    "    for i, count in enumerate(modal_counts):\n",
    "        modal_counts[i] = count/total\n",
    "    return [count_tot/total, modal_counts]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles=\"\"\"Product,Sub-product,Issue,Sub-issue,Company public response,Company,State,ZIP code,Tags,Submitted via,Date sent to company,Company response to consumer,Timely response?,Consumer disputed?\"\"\"\n",
    "titles = titles.split(\",\")\n",
    "\n",
    "#title preprocessing:\n",
    "#for each title, count every single possible response and numerize it. \n",
    "\n",
    "#struct title to list of titles\n",
    "one_hot_lookup={}\n",
    "one_hot_dims={}\n",
    "dims_sum=0\n",
    "names = []\n",
    "for title in titles:\n",
    "    answers=[]\n",
    "    for doc in get_doc_name_iterator(tokens_mini, stemmed=False, condense_form_letters=True):\n",
    "        res = doc[title]\n",
    "        if res not in answers:\n",
    "            answers.append(res)\n",
    "            names.append((title, res))\n",
    "    one_hot_lookup[title]=dict(zip(answers,range(len(answers))))\n",
    "    one_hot_dims[title]=len(answers)\n",
    "    dims_sum+=len(answers)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_d_no_topics = {}\n",
    "name_array=[\"\"]* dims_sum\n",
    "for k in get_doc_name_iterator(tokens_mini, stemmed=False, condense_form_letters=True):\n",
    "    v = k['narrative']\n",
    "    features=[]\n",
    "# deal with linguistic features later.\n",
    "#     features = pronoun_freq_self(v)\n",
    "#     features += pronoun_freq_others(v)\n",
    "#     features.append(hedge_freq(v)[0])\n",
    "#     features.append(negation_freq(v)[0])\n",
    "#     features.append(modal_freq(v)[0])\n",
    "    dims_so_far = 0\n",
    "    feature_vec = [0] * dims_sum\n",
    "    for t in titles:\n",
    "        features.append(one_hot_lookup[t][k[t]]) \n",
    "        feature_vec[dims_so_far+one_hot_lookup[t][k[t]]]=1\n",
    "        name_array[dims_so_far+one_hot_lookup[t][k[t]]]=t+\" = \"+k[t]\n",
    "        dims_so_far+=one_hot_dims[t]\n",
    "     \n",
    "    #features.append(fl_keys.index(k[k.rfind('#') + 1:]))\n",
    "    #flesch kincaid?\n",
    "    #feature_d_no_topics[k] = features\n",
    "    i += 1\n",
    "    feature_d_no_topics[k['complaintID']]=feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 687)\n",
      "(497, 5)\n"
     ]
    }
   ],
   "source": [
    "feat_matrix=[]\n",
    "for k in feature_d_no_topics:\n",
    "    feat_matrix.append(feature_d_no_topics[k])\n",
    "    \n",
    "tsvd = TruncatedSVD(reducedDim)\n",
    "tsvd.fit(feat_matrix)\n",
    "V_tf = tsvd.components_\n",
    "U_tf = tsvd.transform(feat_matrix)\n",
    "print V_tf.shape\n",
    "print U_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "Sub-issue =  , Product = Mortgage , Sub-product = Conventional fixed mortgage , Issue = Loan servicing, payments, escrow account , Consumer disputed? = Yes , Issue = Loan modification,collection,foreclosure , Company = Bank of America , Product = Credit card , Sub-product = Conventional adjustable mortgage (ARM) , Company response to consumer = Closed with explanation , Product = Bank account or service , Tags = Older American , Product = Consumer Loan , Company = Ocwen , Sub-product = FHA mortgage\n",
      "3\n",
      "Submitted via = Web , Timely response? = Yes , Tags =  , Company response to consumer = Closed with explanation , Consumer disputed? = No , Company public response =  , Sub-issue =  , Sub-product =  , Consumer disputed? = Yes , Product = Debt collection , Product = Mortgage , Company public response = Company chooses not to provide a public response , Product = Credit reporting , State = CA , Product = Credit card\n",
      "52\n",
      "Sub-product =  , Company public response = Company chooses not to provide a public response , Product = Credit reporting , Product = Credit card , Issue = Incorrect information on credit report , Consumer disputed? = No , Company = Experian , Company response to consumer = Closed with non-monetary relief , Company = Bank of America , Company = TransUnion Intermediate Holdings, Inc. , Sub-issue =  , Company response to consumer = Closed with monetary relief , Company = Equifax , Issue = Unable to get credit report/credit score , Issue = Credit reporting company's investigation\n",
      "354\n",
      "Company public response =  , Consumer disputed? = No , Product = Credit reporting , Issue = Loan servicing, payments, escrow account , Product = Mortgage , Company = Equifax , Issue = Incorrect information on credit report , Company = TransUnion Intermediate Holdings, Inc. , Sub-product =  , Sub-product = Conventional fixed mortgage , Company = JPMorgan Chase & Co. , State = CA , Company = Ocwen , Date sent to company = 03/30/2015 , State = MA\n",
      "15\n",
      "Consumer disputed? = No , Sub-issue =  , Company public response = Company chooses not to provide a public response , Product = Debt collection , Company response to consumer = Closed with non-monetary relief , Product = Mortgage , Company = Bank of America , Product = Bank account or service , Sub-product = Checking account , Company response to consumer = Closed with monetary relief , Issue = Cont'd attempts collect debt not owed , Sub-issue = Debt is not mine , Sub-product = Conventional fixed mortgage , Issue = Loan servicing, payments, escrow account , Sub-product = I do not know\n"
     ]
    }
   ],
   "source": [
    "topic_words={}\n",
    "for i in range(reducedDim):\n",
    "    row = tsvd_tfidf.components_[i,:]\n",
    "    argsorted = np.argsort(row)[::-1][:15]\n",
    "    scores = row[argsorted]\n",
    "    top_words = [name_array[i] for i in argsorted]\n",
    "    topic_words[i]=' , '.join(top_words)\n",
    "for i in topic_words:\n",
    "    print i\n",
    "    print topic_words[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Topic words: \n",
      "Submitted via = Web , Timely response? = Yes , Tags =  , Company response to consumer = Closed with explanation , Consumer disputed? = No , Company public response =  , Sub-issue =  , Sub-product =  , Consumer disputed? = Yes , Product = Debt collection , Product = Mortgage , Company public response = Company chooses not to provide a public response , Product = Credit reporting , State = CA , Product = Credit card , Sub-product = Conventional fixed mortgage , Issue = Loan servicing, payments, escrow account , Company response to consumer = Closed with non-monetary relief , Issue = Cont'd attempts collect debt not owed , State = FL\n",
      "1\n",
      "Topic words: \n",
      "Sub-issue =  , Product = Mortgage , Sub-product = Conventional fixed mortgage , Issue = Loan servicing, payments, escrow account , Consumer disputed? = Yes , Issue = Loan modification,collection,foreclosure , Company = Bank of America , Product = Credit card , Sub-product = Conventional adjustable mortgage (ARM) , Company response to consumer = Closed with explanation , Product = Bank account or service , Tags = Older American , Product = Consumer Loan , Company = Ocwen , Sub-product = FHA mortgage , Company = JPMorgan Chase & Co. , Company response to consumer = Closed with monetary relief , Issue = Managing the loan or lease , Sub-product = Checking account , Company public response = Company chooses not to provide a public response\n",
      "2\n",
      "Topic words: \n",
      "Sub-product =  , Company public response = Company chooses not to provide a public response , Product = Credit reporting , Product = Credit card , Issue = Incorrect information on credit report , Consumer disputed? = No , Company = Experian , Company response to consumer = Closed with non-monetary relief , Company = Bank of America , Company = TransUnion Intermediate Holdings, Inc. , Sub-issue =  , Company response to consumer = Closed with monetary relief , Company = Equifax , Issue = Unable to get credit report/credit score , Issue = Credit reporting company's investigation , Sub-issue = Account status , Sub-issue = Public record , Issue = Other , Sub-issue = Information is not mine , Tags = \n",
      "3\n",
      "Topic words: \n",
      "Consumer disputed? = No , Sub-issue =  , Company public response = Company chooses not to provide a public response , Product = Debt collection , Company response to consumer = Closed with non-monetary relief , Product = Mortgage , Company = Bank of America , Product = Bank account or service , Sub-product = Checking account , Company response to consumer = Closed with monetary relief , Issue = Cont'd attempts collect debt not owed , Sub-issue = Debt is not mine , Sub-product = Conventional fixed mortgage , Issue = Loan servicing, payments, escrow account , Sub-product = I do not know , Tags = Servicemember , State = VA , Sub-product = Other (i.e. phone, health club, etc.) , Issue = Account opening, closing, or management , Issue = Disclosure verification of debt\n",
      "4\n",
      "Topic words: \n",
      "Company public response =  , Consumer disputed? = No , Product = Credit reporting , Issue = Loan servicing, payments, escrow account , Product = Mortgage , Company = Equifax , Issue = Incorrect information on credit report , Company = TransUnion Intermediate Holdings, Inc. , Sub-product =  , Sub-product = Conventional fixed mortgage , Company = JPMorgan Chase & Co. , State = CA , Company = Ocwen , Date sent to company = 03/30/2015 , State = MA , Product = Student loan , State = GA , Company = Nationstar Mortgage , Issue = Loan modification,collection,foreclosure , Company = Navient Solutions, Inc.\n"
     ]
    }
   ],
   "source": [
    "ref_docs = list(get_doc_name_iterator(tokens_mini, indiv_tokens=False, stemmed=False, condense_form_letters=True))\n",
    "\n",
    "for i, row in enumerate(U_tf.T):\n",
    "    argsort = np.argsort(row)[::-1]\n",
    "    topic_word_sorted = np.argsort(V_tf[i,:])[::-1]\n",
    "    print i\n",
    "    print 'Topic words: '\n",
    "    print ' , '.join([name_array[i] for i in topic_word_sorted[:20]])\n",
    "#     print 'Example 1: ' #for every topic, the highest ranking document.\n",
    "#     print ref_docs[argsort[0]]\n",
    "#     print 'Example 2: '\n",
    "#     print ref_docs[argsort[1]]\n",
    "#     print ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tsne_viz(\n",
    "        mat,\n",
    "        rownames,\n",
    "        colors=None,\n",
    "        output_filename=None,\n",
    "        figheight=40,\n",
    "        figwidth=50):     \n",
    "    \"\"\"2d plot of `mat` using t-SNE, with the points labeled by `rownames`, \n",
    "    aligned with `colors` (defaults to all black).\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    mat : 2d np.array\n",
    "        The matrix to visualize.\n",
    "        \n",
    "    rownames : list of str\n",
    "        Names of the points to visualize.\n",
    "                \n",
    "    colors : list of colornames or None (default: None)\n",
    "        Optional list of colors for rownames. The color names just need to \n",
    "        be interpretable by matplotlib. If they are supplied, they need to \n",
    "        have the same length as rownames, or indices if that is not None. \n",
    "        If `colors=None`, then all the words are displayed in black.\n",
    "      \n",
    "    output_filename : str (default: None)\n",
    "        If not None, then the output image is written to this location. The \n",
    "        filename suffix determines the image type. If None, then \n",
    "        `plt.plot()` is called, with the behavior determined by the \n",
    "        environment.\n",
    "        \n",
    "    figheight : int (default: 40)\n",
    "        Height in display units of the output.\n",
    "            \n",
    "    figwidth : int (default: 50)\n",
    "        Width in display units of the output.\n",
    "        \n",
    "    \"\"\"\n",
    "    indices = list(range(len(rownames)))\n",
    "    # Colors:\n",
    "    if not colors:\n",
    "        colors = ['black' for i in indices]    \n",
    "    # Recommended reduction via PCA or similar:\n",
    "    n_components = 50 if mat.shape[1] >= 50 else mat.shape[1]\n",
    "    dimreduce = PCA(n_components=n_components)\n",
    "    mat = dimreduce.fit_transform(mat)\n",
    "    \n",
    "    # t-SNE:\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)    \n",
    "    tsnemat = tsne.fit_transform(mat) \n",
    "   \n",
    "    # Plot values:\n",
    "    vocab = np.array(rownames)[indices]\n",
    "    xvals = tsnemat[indices, 0] \n",
    "    yvals = tsnemat[indices, 1]\n",
    "    # Plotting:\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "#     fig.set_figheight(40)\n",
    "#     fig.set_figwidth(50)\n",
    "    ax.plot(xvals, yvals, marker='', linestyle='')\n",
    "    # Text labels:\n",
    "    for word, x, y, color in zip(vocab, xvals, yvals, colors):\n",
    "        ax.annotate(word, (x, y), fontsize=8, color=color)\n",
    "    # Output:\n",
    "    if output_filename:\n",
    "        plt.savefig(output_filename, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "\n",
    "def plot_embedding(X, doc_to_topic_matrix, title=None):\n",
    "    #try tsne on clustering\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    labels =  np.argmax(doc_to_topic_matrix, axis=1)\n",
    "    for i in range(X.shape[0]):\n",
    "        \n",
    "        plt.text(X[i, 0], X[i, 1], labels[i],\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in U_tf:\n",
    "    row[0]=-100 #just found that the 0th dimension is garbage - everything falls under it. \n",
    "print U_tf\n",
    "rownames = np.argmax(U_tf, axis=1)#zip(np.argmax(U_tf, axis=1),[item[0] for item in ref_docs])\n",
    "dists = pdist(feat_matrix,'cosine') # or U_tf toarray? no\n",
    "tsne_viz(squareform(dists), rownames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
